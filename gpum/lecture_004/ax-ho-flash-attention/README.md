# Hands-on: lecture_004

Implement FlashAttention according to the its pseudocode. Use block-based tiling for the three inputs and the outputs so that all operations are fused into a single kernel.