{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L_004: Compute and Memory Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At a glance, What do we have to know to keep the GPU busy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S1 - Compute bound:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Terms:\n",
    "\n",
    "- SM (Streaming Multiprocessor)\n",
    "    - **max_threads_block_SM % block_dim == 0**\n",
    "    > A thread block is assigned to one SM. When selecting block dimensions, we must consider the maximum number of threads available per SM. We should choose a block dimension that is divisible by this number to ensure that no threads remain idle.\n",
    "\n",
    "    - We don't have control over which block is scheduled on which SM. On modern GPUs, threads are organized into warps (groups of 32 threads). Additionally, parts of a warp can diverge and execute different instructions, resulting in some threads not running synchronously with others.\n",
    "\n",
    "- Threads, Warps, Blocks \n",
    "  \n",
    "    `threadIdx.x` is the fastest-varying dimension, while the other dimensions (e.g., `threadIdx.y` and `threadIdx.z`) vary more slowly.\n",
    "  ![thread_linearization.png](./ax-images/thread_linearization.png)\n",
    "\n",
    "\n",
    "- **Avoid Divergence Within a Warp:**  \n",
    "  Write conditionals (e.g., `cond ? x[i] : of`) in a way that minimizes branch divergence so that threads within the same warp execute the same instructions.\n",
    "\n",
    "- Avoid FP64/INT64.\n",
    "\n",
    "- **Inspect CUDA Device Properties:**  \n",
    "   - Use `torch.cuda.get_device_properties()` to retrieve detailed information about the GPU's capabilities.\n",
    "   - Even more in [CUDA Docs](https://developer.download.nvidia.com/compute/DevZone/docs/html/C/doc/html/group__CUDART__DEVICE_g5aa4f47938af8276f08074d09b7d520c.html)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2 - Memory bound: Memory architecture and data locality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have seen how threads work and how they are scheduled on the GPU. The second important factor is how memory accesses limit the execution speed of our kernels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tryouts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
